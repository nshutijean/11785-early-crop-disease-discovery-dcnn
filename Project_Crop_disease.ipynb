{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBITN0M_LKds"
   },
   "source": [
    "# PROJECT: Early crop disease detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdoDIKWOMF59"
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dCQlLYldJxX"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1668032554081,
     "user": {
      "displayName": "Sylvan",
      "userId": "10403617381248947394"
     },
     "user_tz": -120
    },
    "id": "Jza7lwiScUhb",
    "outputId": "f965e3d5-f9aa-47bb-8346-10058aaae256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec  8 14:21:41 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   20C    P8    13W /  70W |      2MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # to see what GPU you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10250,
     "status": "ok",
     "timestamp": 1668032569148,
     "user": {
      "displayName": "Sylvan",
      "userId": "10403617381248947394"
     },
     "user_tz": -120
    },
    "id": "bTxfd_nqFnL9",
    "outputId": "6d6754b4-bf7d-4f24-c09a-0017e74ed2a5"
   },
   "outputs": [],
   "source": [
    "!pip install wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/ubuntu/anaconda3/lib/python3.9/site-packages (0.14.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torchvision) (4.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: torch==1.13.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torchvision) (1.13.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torch==1.13.0->torchvision) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torch==1.13.0->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torch==1.13.0->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torch==1.13.0->torchvision) (8.5.0.96)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision) (63.4.1)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision) (0.37.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5062,
     "status": "ok",
     "timestamp": 1668032574203,
     "user": {
      "displayName": "Sylvan",
      "userId": "10403617381248947394"
     },
     "user_tz": -120
    },
    "id": "jwLEd0gdPbSc",
    "outputId": "a1f286ba-1926-4708-f2c9-b8227134bce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "import torchvision #This library is used for image-based operations (Augmentations)\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms, models  # datsets  , transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from warmup_scheduler import GradualWarmupScheduler\n",
    "import glob\n",
    "import wandb\n",
    "from numba import cuda\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30582,
     "status": "ok",
     "timestamp": 1668032604781,
     "user": {
      "displayName": "Sylvan",
      "userId": "10403617381248947394"
     },
     "user_tz": -120
    },
    "id": "SRz9et3SZnbO",
    "outputId": "bd154bdc-8012-4e79-c189-c0a5249b304c"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive # Link your drive if you are a colab user\n",
    "# drive.mount('/content/drive') # Models in this HW take a long time to get trained and make sure to save it her"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oxQNl-YVWHc"
   },
   "source": [
    "# TODOs\n",
    "As you go, please read the code and keep an eye out for TODOs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scOnMklwWBY6"
   },
   "source": [
    "# Download Data from Mendley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47260,
     "status": "ok",
     "timestamp": 1668032658995,
     "user": {
      "displayName": "Sylvan",
      "userId": "10403617381248947394"
     },
     "user_tz": -120
    },
    "id": "3oFjaJTaRjT7",
    "outputId": "d8fdaed7-3388-4d37-c04c-81bbc7faacfc"
   },
   "outputs": [],
   "source": [
    "# !mkdir './content/'\n",
    "# !mkdir './content/data'\n",
    "# # importing libraries\n",
    "import requests, zipfile, io\n",
    "\n",
    "\"\"\"\n",
    "\tDownload images folder from given url, move it to dataset folder.\n",
    "\"\"\"\n",
    "\n",
    "#url for data without augmentation\n",
    "url = \"https://data.mendeley.com/datasets/tywbtsjrjv/1/files/d5652a28-c1d8-4b76-97f3-72fb80f94efc/Plant_leaf_diseases_dataset_without_augmentation.zip?dl=1\"\n",
    "\n",
    "# # url for data with augmentation\n",
    "# url = \"https://data.mendeley.com/datasets/tywbtsjrjv/1/files/b4e3a32f-c0bd-4060-81e9-6144231f2520/Plant_leaf_diseases_dataset_with_augmentation.zip?dl=1\"\n",
    "response = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O68hT27SXClj"
   },
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S7qpMxG0XCJz"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size': 4, # Increase this if your GPU can handle it\n",
    "    'lr': 0.001,\n",
    "    'checkpointPath' : ['checkpoit_CONVNEXT.pth','checkpoint_RESNET.pth'],\n",
    "    'epochs': 10, # 10 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSeiKHYrM-6b"
   },
   "source": [
    "# Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tmRX5omaNDEZ"
   },
   "outputs": [],
   "source": [
    "# DATA_DIR = '/content/Plant_leave_diseases_dataset_with_augmentation'# TODO: Path where you have downloaded the data\n",
    "# # TRAIN_DIR = os.path.join(DATA_DIR, \"classification/train\") \n",
    "# # VAL_DIR = os.path.join(DATA_DIR, \"classification/dev\")\n",
    "# # TEST_DIR = os.path.join(DATA_DIR, \"classification/test\")\n",
    "\n",
    "\n",
    "\n",
    "# # # dataset = CustomDatasetFromCSV(my_path)\n",
    "# # batch_size = 64\n",
    "# # validation_split = .2\n",
    "# # shuffle_dataset = True\n",
    "# # random_seed= 42\n",
    "\n",
    "# # # Creating data indices for training and validation splits:\n",
    "# # dataset_size = len(dataset)\n",
    "# # indices = list(range(dataset_size))\n",
    "# # split = int(np.floor(validation_split * dataset_size))\n",
    "# # if shuffle_dataset :\n",
    "# #     np.random.seed(random_seed)\n",
    "# #     np.random.shuffle(indices)\n",
    "# # train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# # # Creating PT data samplers and loaders:\n",
    "# # train_sampler = SubsetRandomSampler(train_indices)\n",
    "# # valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# # train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "# #                                            sampler=train_sampler)\n",
    "# # validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "# #                                                 sampler=valid_sampler)\n",
    "\n",
    "# # Transforms using torchvision - Refer https://pytorch.org/vision/stable/transforms.html\n",
    "\n",
    "# # train_transforms = torchvision.transforms.Compose([ \n",
    "# #     # Implementing the right transforms/augmentation methods is key to improving performance.\n",
    "# #                     torchvision.transforms.RandAugment(2,14),\n",
    "# #                     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "# # #                     torchvision.transforms.RandomResizedCrop(size=(224,224), scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333)),\n",
    "# #                     torchvision.transforms.ToTensor(),\n",
    "# #                     torchvision.transforms.Normalize(mean=0, std=1, inplace=False)\n",
    "# #                     ])\n",
    "\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor()]\n",
    "# )\n",
    "# # Most torchvision transforms are done on PIL images. So you convert it into a tensor at the end with ToTensor()\n",
    "# # But there are some transforms which are performed after ToTensor() : e.g - Normalization\n",
    "# # Normalization Tip - Do not blindly use normalization that is not suitable for this dataset\n",
    "\n",
    "# # val_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# # train_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR, transform = train_transforms)\n",
    "# # val_dataset = torchvision.datasets.ImageFolder(VAL_DIR, transform = val_transforms)\n",
    "# # # You should NOT have data augmentation on the validation set. Why?\n",
    "\n",
    "# dataset = datasets.ImageFolder(DATA_DIR, transform=transform)\n",
    "\n",
    "# indices = list(range(len(dataset)))\n",
    "# split = int(np.floor(0.85 * len(dataset)))  # train_size\n",
    "# validation = int(np.floor(0.70 * split))  # validation\n",
    "# print(0, validation, split, len(dataset))\n",
    "\n",
    "# train_indices, validation_indices, test_indices = (\n",
    "#     indices[:validation],\n",
    "#     indices[validation:split],\n",
    "#     indices[split:],\n",
    "# )\n",
    "\n",
    "# train_sampler = SubsetRandomSampler(train_indices)\n",
    "# val_sampler = SubsetRandomSampler(validation_indices)\n",
    "# test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "# targets_size = len(dataset.class_to_idx)\n",
    "# # Create data loaders\n",
    "# # train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = config['batch_size'], \n",
    "# #                                            shuffle = True,num_workers = 4, pin_memory = True)\n",
    "# # val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = config['batch_size'], \n",
    "# #                                          shuffle = False, num_workers = 2)\n",
    "\n",
    "# batch_size = 64\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     dataset, batch_size=batch_size, sampler=train_sampler\n",
    "# )\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     dataset, batch_size=batch_size, sampler=test_sampler\n",
    "# )\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     dataset, batch_size=batch_size, sampler=val_sampler\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2YvWtVSEcvWf"
   },
   "outputs": [],
   "source": [
    "transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = 'Plant_leave_diseases_dataset_without_augmentation'\n",
    "image_datasets = {x: datasets.ImageFolder(data_dir, transform=transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val', 'test']}\n",
    "data_size = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SqSR063BGE2e"
   },
   "outputs": [],
   "source": [
    "# You can do this with ImageFolder as well, but it requires some tweaking\n",
    "class ClassificationTestDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, transforms):\n",
    "        self.data_dir   = data_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # This one-liner basically generates a sorted list of full paths to each image in the test directory\n",
    "        self.img_paths  = list(map(lambda fname: os.path.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.transforms(Image.open(self.img_paths[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fVLB41KtGC2o"
   },
   "outputs": [],
   "source": [
    "# Testing the test_dataset and test_loader\n",
    "# test_dataset = ClassificationTestDataset(TEST_DIR, transforms = val_transforms) #Why are we using val_transforms for Test Data?\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = config['batch_size'], shuffle = False,\n",
    "#                          drop_last = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "x4t8eU9gY0Jy"
   },
   "outputs": [],
   "source": [
    "# print(\"Number of classes: \", len(dataset.classes))\n",
    "# print(\"No. of train images: \", dataset.__len__())\n",
    "# print(\"Shape of image: \", dataset[0][0].shape)\n",
    "# print(\"Batch size: \", config['batch_size'])\n",
    "# print(\"Train batches: \", train_loader.__len__())\n",
    "# print(\"Val batches: \", val_loader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1dbBfNa-gcM4"
   },
   "outputs": [],
   "source": [
    "# model = Network().to(device)\n",
    "# summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyIoEbDEdC5F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIqmojPaWD0H"
   },
   "source": [
    "#  Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IqWd5Dbdgoqs"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 7000,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, return_feats = False):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        feats = torch.flatten(x, 1)\n",
    "\n",
    "        out = self.fc(feats)\n",
    "\n",
    "        if return_feats:\n",
    "            return feats\n",
    "        else:\n",
    "            return out\n",
    "             \n",
    "        \n",
    "class Ensemble(nn.Module):\n",
    "    def __init__(self, modelA, modelB):\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x1 = self.modelA(x)\n",
    "        self.x2 = self.modelB(x)\n",
    "        x = torch.cat((self.x1, self.x2), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                   [-1, 39]          79,911\n",
      "================================================================\n",
      "Total params: 23,587,943\n",
      "Trainable params: 23,587,943\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.55\n",
      "Params size (MB): 89.98\n",
      "Estimated Total Size (MB): 377.11\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ResNet50\n",
    "model_resnet = ResNet(Bottleneck, [3,4,6,3], num_classes=39)\n",
    "model_resnet.to(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "summary(model_resnet, (3, 224, 224))\n",
    "checkpoint = torch.load(config['checkpointPath'][1]) #,map_location=torch.device('cpu')\n",
    "model_resnet.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 969,
     "status": "ok",
     "timestamp": 1668033250295,
     "user": {
      "displayName": "Sylvan",
      "userId": "10403617381248947394"
     },
     "user_tz": -120
    },
    "id": "TkbG05UE7eu8",
    "outputId": "e92b0a50-e95a-43b0-da54-dbacf8816cbf"
   },
   "outputs": [],
   "source": [
    "#ConvNext\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, K):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            # conv1\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            # conv2\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            # conv3\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            # conv4\n",
    "            torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.dense_layers = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.4),\n",
    "            torch.nn.Linear(50176, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.4),\n",
    "            torch.nn.Linear(1024, K),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.conv_layers(X)\n",
    "\n",
    "        # Flatten\n",
    "        out = out.view(-1, 50176)\n",
    "\n",
    "        # Fully connected\n",
    "        out = self.dense_layers(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 224, 224]             896\n",
      "              ReLU-2         [-1, 32, 224, 224]               0\n",
      "       BatchNorm2d-3         [-1, 32, 224, 224]              64\n",
      "            Conv2d-4         [-1, 32, 224, 224]           9,248\n",
      "              ReLU-5         [-1, 32, 224, 224]               0\n",
      "       BatchNorm2d-6         [-1, 32, 224, 224]              64\n",
      "         MaxPool2d-7         [-1, 32, 112, 112]               0\n",
      "            Conv2d-8         [-1, 64, 112, 112]          18,496\n",
      "              ReLU-9         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-10         [-1, 64, 112, 112]             128\n",
      "           Conv2d-11         [-1, 64, 112, 112]          36,928\n",
      "             ReLU-12         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-13         [-1, 64, 112, 112]             128\n",
      "        MaxPool2d-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15          [-1, 128, 56, 56]          73,856\n",
      "             ReLU-16          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-17          [-1, 128, 56, 56]             256\n",
      "           Conv2d-18          [-1, 128, 56, 56]         147,584\n",
      "             ReLU-19          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-20          [-1, 128, 56, 56]             256\n",
      "        MaxPool2d-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 256, 28, 28]         295,168\n",
      "             ReLU-23          [-1, 256, 28, 28]               0\n",
      "      BatchNorm2d-24          [-1, 256, 28, 28]             512\n",
      "           Conv2d-25          [-1, 256, 28, 28]         590,080\n",
      "             ReLU-26          [-1, 256, 28, 28]               0\n",
      "      BatchNorm2d-27          [-1, 256, 28, 28]             512\n",
      "        MaxPool2d-28          [-1, 256, 14, 14]               0\n",
      "          Dropout-29                [-1, 50176]               0\n",
      "           Linear-30                 [-1, 1024]      51,381,248\n",
      "             ReLU-31                 [-1, 1024]               0\n",
      "          Dropout-32                 [-1, 1024]               0\n",
      "           Linear-33                   [-1, 39]          39,975\n",
      "================================================================\n",
      "Total params: 52,595,399\n",
      "Trainable params: 52,595,399\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 143.96\n",
      "Params size (MB): 200.64\n",
      "Estimated Total Size (MB): 345.17\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ConvNext\n",
    "targets_size =39\n",
    "model_convNext = CNN(targets_size)\n",
    "model_convNext.to(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "summary(model_convNext, (3, 224, 224))\n",
    "checkpoint = torch.load(config['checkpointPath'][0]) #,map_location=torch.device('cpu')\n",
    "model_convNext.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                   [-1, 39]          79,911\n",
      "          ResNet-175                   [-1, 39]               0\n",
      "          Conv2d-176         [-1, 32, 224, 224]             896\n",
      "            ReLU-177         [-1, 32, 224, 224]               0\n",
      "     BatchNorm2d-178         [-1, 32, 224, 224]              64\n",
      "          Conv2d-179         [-1, 32, 224, 224]           9,248\n",
      "            ReLU-180         [-1, 32, 224, 224]               0\n",
      "     BatchNorm2d-181         [-1, 32, 224, 224]              64\n",
      "       MaxPool2d-182         [-1, 32, 112, 112]               0\n",
      "          Conv2d-183         [-1, 64, 112, 112]          18,496\n",
      "            ReLU-184         [-1, 64, 112, 112]               0\n",
      "     BatchNorm2d-185         [-1, 64, 112, 112]             128\n",
      "          Conv2d-186         [-1, 64, 112, 112]          36,928\n",
      "            ReLU-187         [-1, 64, 112, 112]               0\n",
      "     BatchNorm2d-188         [-1, 64, 112, 112]             128\n",
      "       MaxPool2d-189           [-1, 64, 56, 56]               0\n",
      "          Conv2d-190          [-1, 128, 56, 56]          73,856\n",
      "            ReLU-191          [-1, 128, 56, 56]               0\n",
      "     BatchNorm2d-192          [-1, 128, 56, 56]             256\n",
      "          Conv2d-193          [-1, 128, 56, 56]         147,584\n",
      "            ReLU-194          [-1, 128, 56, 56]               0\n",
      "     BatchNorm2d-195          [-1, 128, 56, 56]             256\n",
      "       MaxPool2d-196          [-1, 128, 28, 28]               0\n",
      "          Conv2d-197          [-1, 256, 28, 28]         295,168\n",
      "            ReLU-198          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-199          [-1, 256, 28, 28]             512\n",
      "          Conv2d-200          [-1, 256, 28, 28]         590,080\n",
      "            ReLU-201          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-202          [-1, 256, 28, 28]             512\n",
      "       MaxPool2d-203          [-1, 256, 14, 14]               0\n",
      "         Dropout-204                [-1, 50176]               0\n",
      "          Linear-205                 [-1, 1024]      51,381,248\n",
      "            ReLU-206                 [-1, 1024]               0\n",
      "         Dropout-207                 [-1, 1024]               0\n",
      "          Linear-208                   [-1, 39]          39,975\n",
      "             CNN-209                   [-1, 39]               0\n",
      "================================================================\n",
      "Total params: 76,183,342\n",
      "Trainable params: 76,183,342\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 430.51\n",
      "Params size (MB): 290.62\n",
      "Estimated Total Size (MB): 721.70\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#ENSEMBLE MODEL\n",
    "model = Ensemble(model_resnet,model_convNext)\n",
    "model.to(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                   [-1, 39]          79,911\n",
      "          ResNet-175                   [-1, 39]               0\n",
      "          Conv2d-176         [-1, 32, 224, 224]             896\n",
      "            ReLU-177         [-1, 32, 224, 224]               0\n",
      "     BatchNorm2d-178         [-1, 32, 224, 224]              64\n",
      "          Conv2d-179         [-1, 32, 224, 224]           9,248\n",
      "            ReLU-180         [-1, 32, 224, 224]               0\n",
      "     BatchNorm2d-181         [-1, 32, 224, 224]              64\n",
      "       MaxPool2d-182         [-1, 32, 112, 112]               0\n",
      "          Conv2d-183         [-1, 64, 112, 112]          18,496\n",
      "            ReLU-184         [-1, 64, 112, 112]               0\n",
      "     BatchNorm2d-185         [-1, 64, 112, 112]             128\n",
      "          Conv2d-186         [-1, 64, 112, 112]          36,928\n",
      "            ReLU-187         [-1, 64, 112, 112]               0\n",
      "     BatchNorm2d-188         [-1, 64, 112, 112]             128\n",
      "       MaxPool2d-189           [-1, 64, 56, 56]               0\n",
      "          Conv2d-190          [-1, 128, 56, 56]          73,856\n",
      "            ReLU-191          [-1, 128, 56, 56]               0\n",
      "     BatchNorm2d-192          [-1, 128, 56, 56]             256\n",
      "          Conv2d-193          [-1, 128, 56, 56]         147,584\n",
      "            ReLU-194          [-1, 128, 56, 56]               0\n",
      "     BatchNorm2d-195          [-1, 128, 56, 56]             256\n",
      "       MaxPool2d-196          [-1, 128, 28, 28]               0\n",
      "          Conv2d-197          [-1, 256, 28, 28]         295,168\n",
      "            ReLU-198          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-199          [-1, 256, 28, 28]             512\n",
      "          Conv2d-200          [-1, 256, 28, 28]         590,080\n",
      "            ReLU-201          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-202          [-1, 256, 28, 28]             512\n",
      "       MaxPool2d-203          [-1, 256, 14, 14]               0\n",
      "         Dropout-204                [-1, 50176]               0\n",
      "          Linear-205                 [-1, 1024]      51,381,248\n",
      "            ReLU-206                 [-1, 1024]               0\n",
      "         Dropout-207                 [-1, 1024]               0\n",
      "          Linear-208                   [-1, 39]          39,975\n",
      "             CNN-209                   [-1, 39]               0\n",
      "================================================================\n",
      "Total params: 76,183,342\n",
      "Trainable params: 76,183,342\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 430.51\n",
      "Params size (MB): 290.62\n",
      "Estimated Total Size (MB): 721.70\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('checkpointdiseasef.pth') #,map_location=torch.device('cpu')\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZCn0qHuZRKj"
   },
   "source": [
    "# Setup everything for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UowI9OcUYPjP"
   },
   "outputs": [],
   "source": [
    "# criterion = torch.nn.CrossEntropyLoss() # TODO: What loss do you need for a multi class classification problem?\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9, weight_decay=1e-4)\n",
    "# # checkpoint = torch.load(config['checkpointPath'])\n",
    "# # model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "# # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.8, patience=1)\n",
    "# # scheduler = torch.optim.lr_scheduler.mulStepLR(optimizer, gamma=0.1, step_size=10)\n",
    "# # scheduler= torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20,30], gamma=0.1, last_epoch=- 1, verbose=False)\n",
    "# cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100, eta_min=0, last_epoch=-1)\n",
    "# scheduler = GradualWarmupScheduler(optimizer, multiplier=8, total_epoch=5, after_scheduler=cosine_scheduler)\n",
    "# # # device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "# # model.cuda()\n",
    "# # TODO: Implement a scheduler (Optional but Highly Recommended)\n",
    "# # You can try ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n",
    "# scaler = torch.cuda.amp.GradScaler() # Good news. We have FP16 (Mixed precision training) implemented for you\n",
    "# # It is useful only in the case of compatible GPUs such as T4/V100\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()  # this include softmax + cross entropy loss\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer_conv = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "# checkpoint = torch.load(config['checkpointPath']) #,map_location=torch.device('cpu')\n",
    "# model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "# optimizer_conv.load_state_dict(checkpoint['optimizer_state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzM11HtcboYv"
   },
   "source": [
    "# Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bgSw6iJJavBZ"
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler):\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(dataloaders['train']), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5) \n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for i,(inputs, labels) in enumerate(dataloaders['train']):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "        batch_bar.set_postfix(\n",
    "            acc=\"{:.04f}%\".format(100 * running_corrects / (config['batch_size']*(i + 1))),\n",
    "            loss=\"{:.04f}\".format(float(running_loss / (i + 1))),\n",
    "            running_corrects=running_corrects,\n",
    "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "        \n",
    "        # scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "        # scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "        # scaler.update() \n",
    "\n",
    "        # TODO? Depending on your choice of scheduler,\n",
    "        # You may want to call some schdulers inside the train function. What are these?\n",
    "      \n",
    "        batch_bar.update() # Update tqdm bar\n",
    "        # clear computation cache\n",
    "        torch.cuda.empty_cache()\n",
    "        # del images\n",
    "        # del labels\n",
    "        # del loss\n",
    "\n",
    "    batch_bar.close() # You need this to close the tqdm bar\n",
    "\n",
    "    # acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
    "    # total_loss = float(total_loss / len(dataloader))\n",
    "    epoch_loss = running_loss / data_size['train']\n",
    "    epoch_acc = 100*running_corrects.double() / data_size['train']\n",
    "\n",
    "    return epoch_acc, epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "m5V2UdnpdEoK"
   },
   "outputs": [],
   "source": [
    "def validate(model, criterion, optimizer, scheduler):\n",
    "  \n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(dataloaders['val']), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
    "\n",
    "    # model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for i,(inputs, labels) in enumerate(dataloaders['val']):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # if phase == 'train':\n",
    "    # scheduler.step()\n",
    "                \n",
    "    # epoch_loss = running_loss / data_size['train']\n",
    "    # epoch_acc = 100*running_corrects.double() / data_size['train']\n",
    "            \n",
    "            # print('{} Loss: {:.4f} Train Acc: {:.4f}'.format(\n",
    "            #     phase, epoch_loss, epoch_acc))\n",
    "\n",
    "        # tqdm lets you add some details so you can monitor training as you train.\n",
    "        batch_bar.set_postfix(\n",
    "            acc=\"{:.04f}%\".format(100 * running_corrects / (config['batch_size']*(i + 1))),\n",
    "            loss=\"{:.04f}\".format(float(running_loss / (i + 1))),\n",
    "            running_corrects=running_corrects,\n",
    "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "        \n",
    "        # scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "        # scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "        # scaler.update() \n",
    "\n",
    "        # TODO? Depending on your choice of scheduler,\n",
    "        # You may want to call some schdulers inside the train function. What are these?\n",
    "      \n",
    "        batch_bar.update() # Update tqdm bar\n",
    "        # clear computation cache\n",
    "        torch.cuda.empty_cache()\n",
    "        # del images\n",
    "        # del labels\n",
    "        # del loss\n",
    "\n",
    "    batch_bar.close() # You need this to close the tqdm bar\n",
    "\n",
    "    # acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
    "    # total_loss = float(total_loss / len(dataloader))\n",
    "    epoch_loss = running_loss / data_size['val']\n",
    "    epoch_acc = 100*running_corrects.double() / data_size['val']\n",
    "\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LccUHgcoe56O"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                batch_bar = tqdm(total=len(dataloaders['train']), dynamic_ncols=True, position=0, leave=False, desc='Train', ncols=5)\n",
    "            else:\n",
    "                model.eval()\n",
    "                batch_bar = tqdm(total=len(dataloaders['val']), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for i,(inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / data_size[phase]\n",
    "            epoch_acc = running_corrects.double() / data_size[phase]\n",
    "            \n",
    "            batch_bar.set_postfix(\n",
    "            acc=\"{:.04f}%\".format(100 * running_corrects / (config['batch_size']*(i + 1))),\n",
    "            loss=\"{:.04f}\".format(float(running_loss / (i + 1))),\n",
    "            running_corrects=running_corrects,\n",
    "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "            \n",
    "#             print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(val_acc, val_loss))\n",
    "\n",
    "            wandb.log({\"train_loss\":epoch_loss, 'train_Acc': epoch_acc})\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "#                 val_=epoch_acc\n",
    "                best_acc = epoch_acc\n",
    "                \n",
    "                # best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print(\"Saving model\")\n",
    "                torch.save({'model_state_dict':model.state_dict(),\n",
    "                            'optimizer_state_dict':optimizer_conv.state_dict(),\n",
    "                            #'scheduler_state_dict':scheduler.state_dict(),\n",
    "                            'epoch_acc': epoch_acc, \n",
    "                            'epoch': epoch}, './checkpointdiseasef.pth')\n",
    "                # best_valacc = val_acc\n",
    "                wandb.save('checkpointdiseasef.pth')\n",
    "                \n",
    "#             print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(epoch_acc, val_loss))\n",
    "            if phase=='val':\n",
    "                wandb.log({'validation_Acc':epoch_acc, \n",
    "                   'validation_loss': epoch_loss})\n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "h1Mt1VD-frLF"
   },
   "outputs": [],
   "source": [
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer_conv = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "cmotca6pcLLY"
   },
   "outputs": [],
   "source": [
    "gc.collect() # These commands help you when you face CUDA OOM error\n",
    "torch.cuda.empty_cache()\n",
    "# free_gpu_cache()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mBgKGkXLrdJ"
   },
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1668033258388,
     "user": {
      "displayName": "Sylvan",
      "userId": "10403617381248947394"
     },
     "user_tz": -120
    },
    "id": "Ix62_BkaLr_D",
    "outputId": "4b513d3e-4fb0-4913-ef43-a0011eda3cbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msntivugu\u001b[0m (\u001b[33mruffers\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"0df4b2e8d09c7bfafcc61974b29dd9ab7ed479b6\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "55c2efbd546747889a9216fa99bfbf7f",
      "e241a0267cbc45e1a70fb339a14a0891",
      "eb849aeb75e3448da504e1e1e65b2dde",
      "4633e9119a1241198efe117aeb39e1f4",
      "a788bcc682144491b1eb29a4a29b705b",
      "1a2731f6ae4d4ac2af8e99edce7a37a9",
      "1d74f30999f5499792179d9bb96a5f1b",
      "09b4dce1c03c435185e8e60cc4de6c0f"
     ]
    },
    "executionInfo": {
     "elapsed": 6457,
     "status": "ok",
     "timestamp": 1668033264833,
     "user": {
      "displayName": "Sylvan",
      "userId": "10403617381248947394"
     },
     "user_tz": -120
    },
    "id": "VG0vmsmbRYEi",
    "outputId": "ed4be219-9c07-4d33-8942-b5e146f7a346"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pzm80k51) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">Ensemble model</strong>: <a href=\"https://wandb.ai/ruffers/Crop_disease_midterm/runs/pzm80k51\" target=\"_blank\">https://wandb.ai/ruffers/Crop_disease_midterm/runs/pzm80k51</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221208_215157-pzm80k51/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pzm80k51). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23c56f633554528b26e9961e758fd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668247349999395, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/deepLearning_project/wandb/run-20221208_215225-35rq47zx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ruffers/Crop_disease_midterm/runs/35rq47zx\" target=\"_blank\">Ensemble model resumed</a></strong> to <a href=\"https://wandb.ai/ruffers/Crop_disease_midterm\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create your wandb run\n",
    "run = wandb.init(\n",
    "    name = \"Ensemble model resumed\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "#     id = '2350zm5y', ###Insert specific run id here if you want to resume a previous run\n",
    "#     resume = True, ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"Crop_disease_midterm\", ### Project should be created in your wandb account \n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQkRw1FvLqYe"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "csVrh81QzzV9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [24:07<?, ?it/s, acc=91.9131%, loss=1.0208, lr=0.0010, running_corrects=tensor(50964, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2552 Acc: 0.9191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:48<?, ?it/s, acc=93.2026%, loss=1.7541, lr=0.0010, running_corrects=tensor(51679, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.4385 Acc: 0.9320\n",
      "Saving model\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [23:41<?, ?it/s, acc=92.2360%, loss=0.9635, lr=0.0010, running_corrects=tensor(51143, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2409 Acc: 0.9224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:50<?, ?it/s, acc=95.4209%, loss=1.0634, lr=0.0010, running_corrects=tensor(52909, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2658 Acc: 0.9542\n",
      "Saving model\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [23:43<?, ?it/s, acc=92.4145%, loss=0.9459, lr=0.0010, running_corrects=tensor(51242, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2365 Acc: 0.9241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:36<?, ?it/s, acc=96.2902%, loss=0.8462, lr=0.0010, running_corrects=tensor(53391, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2115 Acc: 0.9629\n",
      "Saving model\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [23:26<?, ?it/s, acc=92.5462%, loss=0.9325, lr=0.0010, running_corrects=tensor(51315, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2331 Acc: 0.9255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:46<?, ?it/s, acc=96.5319%, loss=0.5453, lr=0.0010, running_corrects=tensor(53525, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1363 Acc: 0.9653\n",
      "Saving model\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [23:47<?, ?it/s, acc=92.9664%, loss=0.9026, lr=0.0010, running_corrects=tensor(51548, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2257 Acc: 0.9297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:32<?, ?it/s, acc=96.4886%, loss=0.9367, lr=0.0010, running_corrects=tensor(53501, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2342 Acc: 0.9649\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [24:12<?, ?it/s, acc=92.8708%, loss=0.8851, lr=0.0010, running_corrects=tensor(51495, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2213 Acc: 0.9287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:47<?, ?it/s, acc=95.6608%, loss=2.4756, lr=0.0010, running_corrects=tensor(53042, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6189 Acc: 0.9566\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [23:55<?, ?it/s, acc=93.0854%, loss=0.8631, lr=0.0001, running_corrects=tensor(51614, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2158 Acc: 0.9309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:33<?, ?it/s, acc=96.3064%, loss=1.1506, lr=0.0001, running_corrects=tensor(53400, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2876 Acc: 0.9631\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [24:00<?, ?it/s, acc=96.4489%, loss=0.4512, lr=0.0001, running_corrects=tensor(53479, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1128 Acc: 0.9645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:37<?, ?it/s, acc=98.4310%, loss=0.4660, lr=0.0001, running_corrects=tensor(54578, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1165 Acc: 0.9843\n",
      "Saving model\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [24:14<?, ?it/s, acc=97.1379%, loss=0.3707, lr=0.0001, running_corrects=tensor(53861, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0927 Acc: 0.9714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:42<?, ?it/s, acc=98.4562%, loss=0.4468, lr=0.0001, running_corrects=tensor(54592, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1117 Acc: 0.9846\n",
      "Saving model\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [23:37<?, ?it/s, acc=97.4138%, loss=0.3372, lr=0.0001, running_corrects=tensor(54014, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0843 Acc: 0.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:46<?, ?it/s, acc=98.6708%, loss=0.4754, lr=0.0001, running_corrects=tensor(54711, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1188 Acc: 0.9867\n",
      "Saving model\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [23:57<?, ?it/s, acc=97.4949%, loss=0.3224, lr=0.0001, running_corrects=tensor(54059, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0806 Acc: 0.9749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:49<?, ?it/s, acc=98.7574%, loss=0.3126, lr=0.0001, running_corrects=tensor(54759, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0782 Acc: 0.9876\n",
      "Saving model\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [24:13<?, ?it/s, acc=97.6464%, loss=0.2949, lr=0.0001, running_corrects=tensor(54143, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0737 Acc: 0.9765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:33<?, ?it/s, acc=99.0153%, loss=0.3188, lr=0.0001, running_corrects=tensor(54902, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0797 Acc: 0.9902\n",
      "Saving model\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [23:56<?, ?it/s, acc=97.6104%, loss=0.3028, lr=0.0001, running_corrects=tensor(54123, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0757 Acc: 0.9761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:47<?, ?it/s, acc=98.6366%, loss=0.4690, lr=0.0001, running_corrects=tensor(54692, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1172 Acc: 0.9864\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [23:53<?, ?it/s, acc=97.7727%, loss=0.2820, lr=0.0000, running_corrects=tensor(54213, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0705 Acc: 0.9777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:27<?, ?it/s, acc=99.0387%, loss=0.3667, lr=0.0000, running_corrects=tensor(54915, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0917 Acc: 0.9904\n",
      "Saving model\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%| | 0/13862 [23:40<?, ?it/s, acc=98.1190%, loss=0.2444, lr=0.0000, running_corrects=tensor(54405, device='cud"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0611 Acc: 0.9812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:   0%| | 0/13862 [05:31<?, ?it/s, acc=99.2750%, loss=0.2850, lr=0.0000, running_corrects=tensor(55046, device='cuda:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0712 Acc: 0.9927\n",
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete in 445m 2s\n",
      "Best val Acc: 0.992750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# free_gpu_cache() \n",
    "\n",
    "model_resnet = train_model(model, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "executionInfo": {
     "elapsed": 1316842,
     "status": "error",
     "timestamp": 1668034581661,
     "user": {
      "displayName": "Sylvan",
      "userId": "10403617381248947394"
     },
     "user_tz": -120
    },
    "id": "EqWO8Edb0BK2",
    "outputId": "53676376-e5f1-4322-d408-d6f097424980"
   },
   "outputs": [],
   "source": [
    "# best_valacc = 0.0\n",
    "\n",
    "# for epoch in range(config['epochs']):\n",
    "\n",
    "#     curr_lr = float(optimizer_conv.param_groups[0]['lr'])\n",
    "\n",
    "#     train_acc, train_loss = train(model, criterion, optimizer_conv, exp_lr_scheduler)\n",
    "#     # train_losses, validation_losses = batch_gd(\n",
    "#     # model, criterion, train_loader, validation_loader, 5)\n",
    "    \n",
    "#     print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n",
    "#         epoch + 1,\n",
    "#         config['epochs'],\n",
    "#         train_acc,\n",
    "#         train_loss,\n",
    "#         curr_lr))\n",
    "    \n",
    "#     val_acc, val_loss = validate(model, criterion, optimizer_conv, exp_lr_scheduler)\n",
    "    \n",
    "#     print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(val_acc, val_loss))\n",
    "\n",
    "#     wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc, \n",
    "#                'validation_loss': val_loss, \"learning_Rate\": curr_lr})\n",
    "    \n",
    "#     # If you are using a scheduler in your train function within your iteration loop, you may want to log\n",
    "#     # your learning rate differently \n",
    "\n",
    "#     # #Save model in drive location if val_acc is better than best recorded val_acc\n",
    "#     if val_acc >= best_valacc:\n",
    "#       #path = os.path.join(root, model_directory, 'checkpoint' + '.pth')\n",
    "#       print(\"Saving model\")\n",
    "#       torch.save({'model_state_dict':model.state_dict(),\n",
    "#                   'optimizer_state_dict':optimizer_conv.state_dict(),\n",
    "#                   #'scheduler_state_dict':scheduler.state_dict(),\n",
    "#                   'val_acc': val_acc, \n",
    "#                   'epoch': epoch}, './drive/MyDrive/checkpointdiseasefv3.pth')\n",
    "#       best_valacc = val_acc\n",
    "#       wandb.save('checkpointdiseasefv2.pth')\n",
    "#       # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpgCHImRkYQW"
   },
   "source": [
    "# Classification Task: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2WQEUjXkWvo"
   },
   "outputs": [],
   "source": [
    "# def test(model,dataloader):\n",
    "\n",
    "#   model.eval()\n",
    "#   batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
    "#   test_results = []\n",
    "  \n",
    "#   for i, (images) in enumerate(dataloader):\n",
    "#       # TODO: Finish predicting on the test set.\n",
    "#       images = images.to(device)\n",
    "\n",
    "#       with torch.inference_mode():\n",
    "#         outputs = model(images)\n",
    "\n",
    "#       outputs = torch.argmax(outputs, axis=1).detach().cpu().numpy().tolist()\n",
    "#       test_results.extend(outputs)\n",
    "      \n",
    "#       batch_bar.update()\n",
    "      \n",
    "#   batch_bar.close()\n",
    "#   return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7R1lcCAzULc"
   },
   "outputs": [],
   "source": [
    "# test_results = test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09b4dce1c03c435185e8e60cc4de6c0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1a2731f6ae4d4ac2af8e99edce7a37a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d74f30999f5499792179d9bb96a5f1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4633e9119a1241198efe117aeb39e1f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55c2efbd546747889a9216fa99bfbf7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e241a0267cbc45e1a70fb339a14a0891",
       "IPY_MODEL_eb849aeb75e3448da504e1e1e65b2dde"
      ],
      "layout": "IPY_MODEL_4633e9119a1241198efe117aeb39e1f4"
     }
    },
    "a788bcc682144491b1eb29a4a29b705b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e241a0267cbc45e1a70fb339a14a0891": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a788bcc682144491b1eb29a4a29b705b",
      "placeholder": "",
      "style": "IPY_MODEL_1a2731f6ae4d4ac2af8e99edce7a37a9",
      "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "eb849aeb75e3448da504e1e1e65b2dde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d74f30999f5499792179d9bb96a5f1b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_09b4dce1c03c435185e8e60cc4de6c0f",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
